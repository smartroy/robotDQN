{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #deep learning lib\n",
    "import numpy as np  #handle matrices\n",
    "import robotEnvGen #robot sim environment\n",
    "import pickle\n",
    "from robot_sim_vm_vp_dynamic_drate_w import S1Wrapper as S1Wrapper # s1 classifier lib\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(sim_time):\n",
    "    periods = [1,2,3,5] # available periods\n",
    "    motors = [150,250,400,500] #available motors\n",
    "    sss_clf = 'sk20_clf/sss_p1_e20_NNClf.p' # sss classifier for safety assurance\n",
    "    sss_scaler = 'sk20_clf/sss_p1_e20_ClfScaler.p'\n",
    "    env = 20 # disturbance magnitude- maximum\n",
    "    s1_clf=[] #s1 classifier holders\n",
    "    for i in periods: # fill s1 classifier holder with classifier and other parameters for each period and motor\n",
    "        clf_p = {'period':i, 'clfs_m':[]}\n",
    "        for j in motors:\n",
    "            A = np.genfromtxt('dis_para/dis-paraAp'+str(i)+'.csv',delimiter=',')\n",
    "            B = np.genfromtxt('dis_para/dis-paraBp'+str(i)+'.csv',delimiter=',')\n",
    "            clf = pickle.load(open('sk20_clf/sss_p'+str(i)+'_e'+str(env)+'_NNClf.p','rb'))\n",
    "            clf_scaler = pickle.load(open('sk20_clf/sss_p'+str(i)+'_e'+str(env)+'_ClfScaler.p','rb'))\n",
    "            s1clf = S1Wrapper(A, B, j, env, clf, clf_scaler)\n",
    "            clf_p['clfs_m'].append({'motor':j, 'clf':s1clf})\n",
    "        s1_clf.append(clf_p)\n",
    "    h = 0.001 #step size in second\n",
    "    initial_state = np.asarray([[-0.2,0.1,-0.1,4.0,-3.5,-4.0]])\n",
    "    robot_env = robotEnvGen.robotEnv(h,sss_clf,sss_scaler,s1_clf,initial_state)\n",
    "    return robot_env, robot_env.periods # sim_env and action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 2\n",
    "sim, sim_action = create_environment(sim_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = [6] #input is the current physical state\n",
    "action_size = len(sim_action)\n",
    "learning_rate = 0.0002 # alpha\n",
    "\n",
    "#### training parameters\n",
    "total_episodes = 500 # total no. of episode\n",
    "batch_size = 64\n",
    "\n",
    "##### exploration parameter for epsilon greedy strategy\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.0001\n",
    "\n",
    "#Q learning parameter\n",
    "gamma = 0.95 # discounting rate\n",
    "\n",
    "#memory parameters\n",
    "pertain_length = batch_size # number of experience stored in memory when initized\n",
    "memory_size = 1000000 # no. of experience kept\n",
    "\n",
    "training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetworks'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name='inputs')\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name='actions_')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target')\n",
    "            self.fc1 = tf.layers.dense(inputs = self.inputs_,\n",
    "                                units = state_size[0]*2,\n",
    "                                activation = tf.nn.relu,\n",
    "                                kernel_initializer =tf.contrib.layers.xavier_initializer(), name='fc1')\n",
    "            \n",
    "            self.fc2 = tf.layers.dense(inputs = self.fc1,\n",
    "                                units = action_size,\n",
    "                                activation = tf.nn.relu,\n",
    "                                kernel_initializer =tf.contrib.layers.xavier_initializer(), name='fc2')\n",
    "            self.output = tf.layers.dense(inputs = self.fc2,\n",
    "                                units = action_size,\n",
    "                                activation = None,\n",
    "                                kernel_initializer =tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arrange(buffer_size), size = batch_size, replace = false)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)\n",
    "sim.reset(np.asarray([[-0.2,0.1,-0.1,4.0,-3.5,-4.0]]))\n",
    "for i in range(pertain_length):\n",
    "    if i==0:\n",
    "        state = sim.robot.state\n",
    "    action = random.choice(sim_action)\n",
    "    next_state, reward, done = sim.period_run(1,action)\n",
    "    memory.add((state,action, reward, next_state, done))\n",
    "    if done:\n",
    "        sim.reste(np.asarray([[-0.2,0.1,-0.1,4.0,-3.5,-4.0]]))\n",
    "        state = sim.get_state()\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.196    ,  0.0965   , -0.104    ,  3.9881067, -3.4676091,\n",
       "        -4.0279389]),\n",
       " 3,\n",
       " 0.21961252776465076,\n",
       " array([-0.19201189,  0.09303239, -0.10802794,  3.97652669, -3.43591778,\n",
       "        -4.05562767]),\n",
       " 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
